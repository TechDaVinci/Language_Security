{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import drive\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mieCrkwwMUgJ",
        "outputId": "3a378b69-7be9-48d1-9349-a3c5096f7d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/358.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/358.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "drive.mount('/content/drive')\n",
        "dataset = '/content/drive/MyDrive/Colab/train_hindi_mod.csv'\n",
        "df1 = pd.read_csv(dataset)\n",
        "\n",
        "en_hn_dict = '/content/drive/MyDrive/Colab/hindi_english_mod2.csv'\n",
        "df_trans = pd.read_csv(en_hn_dict)\n",
        "\n",
        "bad_words = '/content/drive/MyDrive/Colab/bad_words.csv'\n",
        "with open(bad_words, 'r') as file:\n",
        "    offensive_words = file.read().split(',')\n",
        "\n",
        "# converting dataframe to dictionary\n",
        "post_dict = df1.to_dict('records')\n",
        "trans_dict = df_trans.to_dict('records')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA3EivphNBFV",
        "outputId": "8a170dd1-b805-47ae-f72b-312efe2fbff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dictionary for hindi to english\n",
        "dictionary = {}\n",
        "\n",
        "for row in trans_dict:\n",
        "    if row['hindi'] not in dictionary:\n",
        "        dictionary[row['hindi']] = [str(row['english'])]\n",
        "    else:\n",
        "        dictionary[row['hindi']].append(str(row['english']))\n",
        "print(\"Dictionary size: \", len(dictionary))"
      ],
      "metadata": {
        "id": "JxKwfJPfX6z3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949147eb-1441-4e12-c51b-b3865da8be01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary size:  32680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIpGb1wLMFBl"
      },
      "outputs": [],
      "source": [
        "# preprocessing the data\n",
        "def preprocess_sentence(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stem each word\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatize each word\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "    # Join the tokens back into a sentence\n",
        "    cleaned_sentence = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filtertext(sentnc):\n",
        "    sentnc = re.sub(r'\\bhttp\\S+', '', sentnc) # Remove links\n",
        "    # text = re.sub(r'\\B#\\w+', '', text) # Remove hashtags\n",
        "    sentnc = re.sub(r'\\B@\\w+', '', sentnc) # Remove mentions\n",
        "    sentnc = emoji.demojize(sentnc) # Replace emojis with text representation\n",
        "    sentnc = re.sub(r':[a-z_]+:', '', sentnc) # Remove emoji codes\n",
        "    words = re.split(r'[-, :0-9\\n?\\'\\\"]+', sentnc) # splitting based on delimiter\n",
        "    words = list(filter(lambda word: word != \"\", words))\n",
        "    sentnc = ' '.join(words)\n",
        "    return sentnc"
      ],
      "metadata": {
        "id": "zd9BVraHZsj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#translating the sentence\n",
        "def translate(sentence):\n",
        "    global dictionary\n",
        "    words = sentence.split()\n",
        "    new_sentence = ''\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            new_sentence += word + ' '\n",
        "        else:\n",
        "            # for i in range(min(4, len(dictionary[word]))):\n",
        "            for meaning in dictionary[word]:\n",
        "                new_sentence += meaning + ' '\n",
        "    return new_sentence.rstrip()\n",
        "\n",
        "\n",
        "# translating and expanding the query\n",
        "def transexpand(sentence):\n",
        "    global dictionary\n",
        "    words = sentence.split()\n",
        "    new_sentence = ''\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            new_sentence += word + ' '\n",
        "        else:\n",
        "            for i in range(min(4, len(dictionary[word]))):\n",
        "                new_sentence += dictionary[word][i] + ' '\n",
        "    return new_sentence.rstrip()"
      ],
      "metadata": {
        "id": "JbxqkXXGN3ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data in test and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(df1['Post'], df1['Labels Set'], test_size=0.2, random_state= 9)\n",
        "\n",
        "# filtering, translating and preprocessing the training data\n",
        "X_train = X_train.apply(filtertext)\n",
        "X_train = X_train.apply(translate)\n",
        "X_train = X_train.apply(preprocess_sentence)\n",
        "\n",
        "\n",
        "# initialising the lists and dicts\n",
        "class_docs_totals = {} # to store how many docs a word appeared in a class\n",
        "class_word_counts = {} # to sotre how many times a word appeared in a class\n",
        "class_word_totals = {} # total no. of words in a class\n",
        "class_doc_counts = {} # no. of doc in a class containing the word\n",
        "class_priors = {} # probability of a class\n",
        "vocab = set() # vocabulary generated from the training data\n",
        "\n",
        "# print the training data metadata\n",
        "count_1 = list(y_train).count(1)\n",
        "count_0 = len(y_train) - count_1\n",
        "print(\"Training data size: \", len(y_train))\n",
        "print(\"Label Imbalance: \", count_1 / count_0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgvbUSa7-oS4",
        "outputId": "276d12bd-9813-45e2-b0f7-dd2588025f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size:  4582\n",
            "Label Imbalance:  0.297281993204983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the method Used:  score = Summation of prob\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict2(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        log_prob = 0\n",
        "        # log_prob = 1;\n",
        "        for word in words:\n",
        "          count = 1  # Laplace smoothing\n",
        "          if word in vocab:\n",
        "              count += class_doc_counts[c][word]\n",
        "          log_prob += np.log(count / (class_docs_totals[c] + len(vocab)))\n",
        "        probs[c] = log_prob\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "n2T1IVxMs_Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the method Used:  score = Summation of prob with prior\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict3(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    # text = preprocess_sentence(text)\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        log_prob = np.log(class_priors[c])\n",
        "        # log_prob = 1;\n",
        "        for word in words:\n",
        "          count = 1  # Laplace smoothing\n",
        "          if word in vocab:\n",
        "              count += class_word_counts[c][word]\n",
        "          log_prob += np.log(count / (class_word_totals[c] + len(vocab)))\n",
        "        probs[c] = log_prob\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "6ZC6ke60RanW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the method Used:  score = Summation of (1 + log(tf))\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        score = 0\n",
        "        for word in words:\n",
        "          if word in vocab:\n",
        "            if class_word_counts[c][word] != 0:\n",
        "              score += (1 + np.log(class_word_counts[c][word]))\n",
        "            else:\n",
        "              score += 0\n",
        "        probs[c] = score\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "8hOZf1-OiTKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basemodel():\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    # evaluting the vocabulary\n",
        "    for sentence in X_train:\n",
        "        words = sentence.split()\n",
        "        for word in words:\n",
        "            vocab.add(word)\n",
        "\n",
        "    print(\"Vocab size: \", len(vocab))\n",
        "\n",
        "    # Create a dictionary to store the count of each word in each class\n",
        "    # array of index = word for all labels\n",
        "    # count how many time a word occured in each labels\n",
        "    y_train_unique = np.unique(y_train)\n",
        "    for c in y_train_unique:\n",
        "        class_doc_counts[c] = {}\n",
        "        class_word_counts[c] = {}\n",
        "        for word in vocab:\n",
        "            class_doc_counts[c][word] = 0\n",
        "            class_word_counts[c][word] = 0\n",
        "\n",
        "    # Count the number of occurrences of each word in each class\n",
        "    for sent, c in zip(X_train, y_train):\n",
        "        words = sent.split()\n",
        "        for word in words:\n",
        "            class_word_counts[c][word] += 1\n",
        "        for word in np.unique(words):\n",
        "            class_doc_counts[c][word] += 1\n",
        "\n",
        "    for c in np.unique(y_train):\n",
        "        class_priors[c] = (len(y_train[y_train == c]) + 1) / (len(y_train) + len(np.unique(y_train)))\n",
        "\n",
        "    for c in np.unique(y_train):\n",
        "        class_docs_totals[c] = sum(class_doc_counts[c].values())\n",
        "\n",
        "    # Compute the total count of words in each class\n",
        "    for c in y_train_unique:\n",
        "        class_word_totals[c] = sum(class_word_counts[c].values())"
      ],
      "metadata": {
        "id": "0ZrgqqB0iiGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel()\n",
        "correct = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for sen, off in zip(X_test, y_test):\n",
        "    pred = predict(sen)\n",
        "    if pred == 1:\n",
        "      if(off == 1):\n",
        "        tp += 1\n",
        "        correct += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    else:\n",
        "      if(off == 0):\n",
        "        correct += 1\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "accuracy = correct / len(y_test)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Percision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "print(\"F1 score:\", f1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T1vuauCBCw2",
        "outputId": "2686b019-c1e0-48c6-a23f-b344cddde31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size:  21259\n",
            "Accuracy: 0.8219895287958116\n",
            "Percision: 0.7317073170731707\n",
            "Recall: 0.42857142857142855\n",
            "F1 score: 0.5405405405405405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict('''#जीवनसंवाद: हम संघर्ष करना चाहते हैं, क्योंकि सामने संघर्ष की मिसाल रखना चाहते हैं. नए शहर में इतना कुछ नया होगा कि हमें अपनी जड़ें जमाने में बरसों बीत जाएंगे. हम कर्ज में जरूर हैं, लेकिन हम हारना नहीं चाहते. वह भी बिना लड़े.\n",
        "#JeevanSamvad\n",
        "@DayashankarMi\n",
        "https://t.co/nd7troF0JF https://t.co/CmAzq2Mj8S'''))\n",
        "print(predict('''मोदीजी और जब सारा देश सेना के साथ खडी है,पर दो सयाने विदेश मे पडे है?🤔 इसलिए बोलते हैं विदेशी मां का बेटा कभी देशभक्त न'''))\n",
        "print(predict('''she is a bloody bitch'''))"
      ],
      "metadata": {
        "id": "5vJ8gZCMb1Nl",
        "outputId": "93ee26ee-8302-457e-b58d-a5a5a30b59d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentnc = '''भारत ने कहा, चीन के सैनिकों के साथ लद्दाख में झड़प, क्या कहना है चीन का?\n",
        "स्टोरी: टीम बीबीसी\n",
        "आवाज़: भरत शर्मा https://t.co/Re6GyZVmbY'''\n",
        "sentnc = filtertext(sentnc)\n",
        "print(sentnc)\n",
        "sentnc = translate(sentnc)\n",
        "print(sentnc)\n",
        "sentnc = preprocess_sentence(sentnc)\n",
        "print(sentnc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYDxKLHFonLS",
        "outputId": "80176012-2639-46ae-ce58-0e94b2edf1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "भारत ने कहा चीन के सैनिकों के साथ लद्दाख में झड़प क्या कहना है चीन का स्टोरी टीम बीबीसी आवाज़ भरत शर्मा\n",
            "india ने told said tattle china from k सैनिकों from k society along company accompaniment companionship लद्दाख into during through on under at throughout by over in within with to among towards inside skirmish altercation tangle which what have it utter phrase cry declare answer bid run by talk provide mention volunteer say spell out advertise drop speak up for call tell breathe speak to crack rap out take up with run past speak \n",
            "to address raise speak call on observe communicate have china in am  iana of स्टोरी team बीबीसी voice song alauda arvensis skylark alauda \n",
            "arvensis शर्मा\n",
            "india ने told said tattl china k सैनिकों k societi along compani accompani companionship लद्दाख throughout within among toward insid skirmish alterc tangl utter phrase cri declar answer bid run talk provid mention volunt say spell advertis drop speak call tell breath speak crack rap take run past speak address rais speak call observ commun china iana स्टोरी team बीबीसी voic song alauda arvensi skylark alauda arvensi शर्मा\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def detect(query):\n",
        "    \"\"\"\n",
        "    Detects offensive queries in Hindi using TextBlob.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query in Hindi.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the query is offensive, False otherwise.\n",
        "    \"\"\"\n",
        "    # Create a TextBlob object for the input query\n",
        "    blob = TextBlob(query)\n",
        "\n",
        "    # Check for offensive language using TextBlob's sentiment polarity\n",
        "    polarity = blob.sentiment.polarity\n",
        "\n",
        "    # If polarity is less than 0, the query is considered offensive\n",
        "    if polarity < 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "correct = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for sen, off in zip(X_test, y_test):\n",
        "    pred = max(predict(sen), detect(sen))\n",
        "    if pred == 1:\n",
        "      if(off == 1):\n",
        "        tp += 1\n",
        "        correct += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    else:\n",
        "      if(off == 0):\n",
        "        correct += 1\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "accuracy = correct / len(y_test)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Percision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "print(\"F1 score:\", f1_score)"
      ],
      "metadata": {
        "id": "bjuHTpM8U9en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}